/home/sebastian/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py:974: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
tensor([[-0.0245,  0.2132,  0.0220, -0.2795]], device='cuda:0') torch.Size([1, 4])
tensor([[-0.0203,  0.0178,  0.0164,  0.0201]], device='cuda:0') torch.Size([1, 4])
tensor([[-0.0199, -0.1776,  0.0168,  0.3179]], device='cuda:0') torch.Size([1, 4])
tensor([[-0.0235, -0.3729,  0.0232,  0.6158]], device='cuda:0') torch.Size([1, 4])
tensor([[-0.0309, -0.5684,  0.0355,  0.9157]], device='cuda:0') torch.Size([1, 4])
tensor([[-0.0423, -0.7639,  0.0538,  1.2193]], device='cuda:0') torch.Size([1, 4])
tensor([[-0.0576, -0.9597,  0.0782,  1.5284]], device='cuda:0') torch.Size([1, 4])
tensor([[-0.0768, -1.1557,  0.1088,  1.8444]], device='cuda:0') torch.Size([1, 4])
tensor([[-0.0999, -1.3518,  0.1456,  2.1688]], device='cuda:0') torch.Size([1, 4])
tensor([[-0.1269, -1.5480,  0.1890,  2.5026]], device='cuda:0') torch.Size([1, 4])
Episode: 1 | Reward: 11.0 | Q Loss: -3.595121383666992 | Steps: 11
